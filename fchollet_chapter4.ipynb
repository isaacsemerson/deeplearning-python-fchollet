{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/Ua/kAyyrxdS6G1sv1ZAU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacsemerson/deeplearning-python-fchollet/blob/main/fchollet_chapter4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pzUumUl_5w5X"
      },
      "outputs": [],
      "source": [
        "# run this before anything else\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 4.1 - This loads the imdb reviews dataset from keras. This set contains 50000 reviews, split between the training and test sets. Half of the reviews are positive, the other half are negative.\n",
        "\n",
        "num_words within load_data() will keep only the top amount of num_words occuring in the dataset. With the normal word variety we would be training on too many unique words, making classification very hard."
      ],
      "metadata": {
        "id": "xn6tjWLW_7pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n",
        "print(\"First sample decode:\", decoded_review)"
      ],
      "metadata": {
        "id": "d44Q5_K2Gs7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4d55de-e50f-4393-a238-d5d1e1445e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sample decode: ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 4.2 - This is how you would translate a review to English. get_word_index() returns a dictionary of words, mapped by index. We then flip the dictionary as we want to get words (original key) based on index (original value). The \"i - 3\" is special to this dataset. The first 3 indicies (0-2) are reserved for special utilities."
      ],
      "metadata": {
        "id": "NnDLtdU2Puyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for sentenceIndex, sentence in enumerate(sequences):\n",
        "    for wordIndex in sentence:\n",
        "      results[sentenceIndex, wordIndex] = 1\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "print(\"Shape of vectorized training data:\", x_train.shape)\n",
        "y_train = np.asarray(train_labels).astype(\"float32\")\n",
        "y_test = np.asarray(test_labels).astype(\"float32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I09EHh9pKsl",
        "outputId": "c37f20fb-b0d1-4290-a4c0-c75c9668241b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of vectorized training data: (25000, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 4.3 - We need to feed our neural network a standard data length/type. The default data has a variable sentence length, so above we are turning each sentence into a vector row. Each column represents the index of the word in our dictionary. If that word is included in the sentence, the corresponding index (column) is marked by a 1."
      ],
      "metadata": {
        "id": "TxztdAGwrJet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "WMNENNSzrmd4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 4.4 - Defining our model. The book holds our hand here and says it will explain more in chapter 5, but for now we are told that a stack of dense layers works well on binary classifcation. With that being said, a stack of dense layers requires us to decide how many layers and what the output units will be. However, the book also decides this, reserving explaination until next chapter.\n",
        "\n",
        "Activations:\n",
        "- Relu, this essentially squishes negatve numbers to 0 (think of an xy axis, a relu activation will prevent y from ever being negative)\n",
        "- Sigmoid, this adjusts the value to a probability curve of the value being 1 (positive review in our case)"
      ],
      "metadata": {
        "id": "Jn5U1RrS0wmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "lMdXE62N1avQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 4.5 - Here we are compiling the model.\n",
        "\n",
        "Optimizer - Book advises us to use rmsprop as an optimizer as it is a good default choice for most models.\n",
        "Loss - The book advises us to go with crossentropy as it is \"usually the best choice for binary classification\". This loss function compares the distance between probability distributions for a batch of predictions and targets (example, 60% of this batch is positive for targets but only 40% of our predictions are positive).\n",
        "Metrics - Accuracy here as a default (an abstract of the loss value)."
      ],
      "metadata": {
        "id": "XYwQ_WG-4ZUc"
      }
    }
  ]
}