{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNm7FkIapfRaroGSupIuNok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacsemerson/deeplearning-python-fchollet/blob/main/fchollet_chapter3_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y8PSF1jTnI7o"
      },
      "outputs": [],
      "source": [
        "# Run these imports before running anything else.\n",
        "# I have not tested the code in this file. These were individual examples in the book.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDense(keras.layers.Layer):\n",
        "  def __init__(self, units, activation=None):\n",
        "    super().__init__()\n",
        "    self.units = units\n",
        "    self.activation = activation\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "    self.W = self.add_weight(shape=(input_dim, self.units), initializer=\"random_normal\")\n",
        "    self.b = self.add_weight(shape=(self.units,), initializer=\"zeros\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    y = tf.matmul(inputs, self.W) + self.b\n",
        "    if self.activation is not None:\n",
        "      y = self.activation(y)\n",
        "    return y\n",
        "\n",
        "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
        "input_tensor = tf.ones(shape=(2, 784))\n",
        "output_tensor = my_dense(input_tensor)\n",
        "print(\"Shape of output tensor from SimpleDense implementation:\", output_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "567GGqmAqxdB",
        "outputId": "a2315ae9-302f-434f-d9d2-fe78fec98103"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of output tensor from SimpleDense implementation: (2, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 3.22 - Here we are building a \"Dense\" layer, deriving from the Keras layers API. We have a build function to generate the weights, and a call function to perform the actual computation (forward pass). I believe these functions are called from within the Keras API?\n",
        "\n",
        "The \"matmul(input, W)\" part of the calculation influences the output shape. As you see in the self.W creation, our W shape is a combination of the last dimension of our input tensor, and the output shape (\"units\" as passed to the constructor). Dot product can be visualized as two rectangles combining into one, with the result being a tensor shaped after the input rows (samples) and output size (units). Chapter 2 has a great diagram for this."
      ],
      "metadata": {
        "id": "HRKNG3FfvDtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __call__(self, inputs):\n",
        "  if not self.built:\n",
        "    self.build(inputs.shape)\n",
        "    self.built = True\n",
        "  return self.call(inputs)"
      ],
      "metadata": {
        "id": "qE3f_1GsvW44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 3.22b - This is a general idea of how the base layer call function works. As expected above, we reference both build and call. This allows us to perform JIT state registration when the layer is called within the model.\n",
        "\n",
        "One benefit of JIT state is automatic inference of the layer's input shape. Chapter 2 showed a basic Dense layer implementation where the weights were initalized within the constructor (NaiveDense). The problem with this is that you need to know the previous model weights before compile time, which can be difficult in complex scenarios. The book tells us to think of layers as lego bricks (if that helps, the input needs to match the last output basically)."
      ],
      "metadata": {
        "id": "ia0a3ufq0Ens"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Squential([keras.layers.Dense(1)])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"mean_squared_error\",\n",
        "              accuracy=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "GFFIcDAf3OmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 3.22c - Here is an example of the compile step for a keras-backed model. We pass in three parameters:\n",
        "- Optimizer, this is what moves the weights in a direction reducing loss (main part of backward pass, training the model). So far this has been variants of SGD.\n",
        "- Loss, this calculates how far away the predictions are from results. (in the case above, we subtract the predictions from results and get a mean value)\n",
        "- Accuracy, Metrics to determine how successful you are. Biggest difference between accuracy and loss is that the model does not optimize for accuracy.\n",
        "\n",
        "These strings are references to functions. You can pass in your own functions. There are also parameters for some of the keras functions (such as `keras.optimizers.RMSprop(learning_rate)`)."
      ],
      "metadata": {
        "id": "IzkJ-A5PTA4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will not work without inputs and targets (need sample data)\n",
        "history = model.fit(\n",
        "    inputs,\n",
        "    targets,\n",
        "    epochs=5,\n",
        "    batch_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "1P6C4cUwcRGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing 3.22d - This is the training loop. Essentially the last step to calling a model at runtime. We pass in the input and target arrays, assign an epoch (how many times through the data), and a batch size to pull from."
      ],
      "metadata": {
        "id": "Eoz_mvY3cv6o"
      }
    }
  ]
}